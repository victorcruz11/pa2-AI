{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2253e02f",
   "metadata": {},
   "source": [
    "## Exercise 2.8\n",
    "\n",
    "Implement a performance-measuring environment simulator for the vacuum-cleaner world depicted in Figure 2.8 and specified on page . Your implementation should be modular so that the sensors, actuators, and environment characteristics (size, shape, dirt placement, etc.) can be changed easily. (Note: for some choices of programming language and operating system there are already implementations in the online code repository.)\n",
    "\n",
    "For this we developed the following code, we can change the type of agent by replacing `agent = ReflexVacuumAgent()` for the following other agents: RandomVacuumAgent, TableDrivenVacuumAgent, and ModelBasedVacuumAgent.\n",
    "\n",
    "As well we can change the status of the environment by replacing <font color=orange>'Dirty'</font> for <font color=green>'Clean'</font> and viceversa.\n",
    "\n",
    "The performance of the agent is mesured by a point system. The performance increases when the status of the environment is set to clean for 10 points. Otherwise, the performance decreases the more the agent moves, the penalty is -1 point.\n",
    "\n",
    "For a 10 step cycle, the max score an agent can achieve is 19, assuming both locations are dirty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f4812c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T18:54:15.840319Z",
     "start_time": "2024-03-07T18:54:15.726069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function ReflexVacuumAgent.<locals>.program at 0x000001E48D7BE7A0>\n"
     ]
    }
   ],
   "source": [
    "from agents import *\n",
    "from xy_vacuum_environment import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1d8fb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T18:54:16.811997Z",
     "start_time": "2024-03-07T18:54:16.805935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Agent> perceives ((0, 0), 'Dirty') and does Suck @ (0, 0)\n",
      "<Agent> perceives ((0, 0), 'Clean') and does Right @ (0, 0)\n",
      "<Agent> perceives ((1, 0), 'Dirty') and does Suck @ (1, 0)\n",
      "<Agent> perceives ((1, 0), 'Clean') and does Left @ (1, 0)\n",
      "<Agent> perceives ((0, 0), 'Clean') and does Right @ (0, 0)\n",
      "<Agent> perceives ((1, 0), 'Clean') and does Left @ (1, 0)\n",
      "<Agent> perceives ((0, 0), 'Clean') and does Right @ (0, 0)\n",
      "<Agent> perceives ((1, 0), 'Clean') and does Left @ (1, 0)\n",
      "<Agent> perceives ((0, 0), 'Clean') and does Right @ (0, 0)\n",
      "<Agent> perceives ((1, 0), 'Clean') and does Left @ (1, 0)\n",
      "Performance for vacuum Agent: 12\n",
      "Status of Environment end of cycle: {(1, 0): 'Clean', (0, 0): 'Clean'}\n"
     ]
    }
   ],
   "source": [
    "# Creating our simple Agent: A reflex agent for the two-state vacuum environment.\n",
    "agent = ReflexVacuumAgent()\n",
    "\n",
    "# Creating the environment for our agent\n",
    "env = TrivialVacuumEnvironment()\n",
    "\n",
    "# This sets the status for the environment\n",
    "env.status = {(1, 0): 'Dirty', (0, 0): 'Dirty'}\n",
    "\n",
    "# Adding the agent to the environment and wrapping it with TraceAgent to see its actions\n",
    "env.add_thing(TraceAgent(agent))\n",
    "\n",
    "# Runs the environment, we can add a required amount of steps for the agent to take. In this case, we chose 10.\n",
    "env.run(10)\n",
    "\n",
    "# Measurement of performance for each agent\n",
    "print(\"Performance for vacuum Agent: \" + str(agent.performance))\n",
    "\n",
    "# Status of environment at the end of the cycle\n",
    "print(\"Status of Environment end of cycle: {}\".format(env.status))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f194a81",
   "metadata": {},
   "source": [
    "## Exercise 2.11\n",
    "\n",
    "Consider a modified version of the vacuum environment in Exercise 2.10, in which the geography of the environment—its extent, boundaries, and obstacles—is unknown, as is the initial dirt configuration. (The agent can go Up and Down as well as Left and Right.)\n",
    "\n",
    "1. Can a simple reflex agent be perfectly rational for this environment? Explain.\n",
    "\n",
    "No, a simple reflex agent cannot be perfectly rational for this environment where the geography, boundaries, obstacles, and initial dirt configuration are unknown. A simple reflex agent bases its actions solely on the current percept (the dirt sensor readings and current location). Without knowledge of the environment's layout and dirt configuration, the agent may make suboptimal decisions or get stuck in loops, as it lacks the capability to reason about its environment beyond the immediate sensory input.\n",
    "    \n",
    "2. Can a simple reflex agent with a randomized agent function outperform a simple reflex agent? Design such an agent and measure its performance on several environments.\n",
    "\n",
    "Yes, a simple reflex agent augmented with randomization has the potential to outperform a standard simple reflex agent. While the simple reflex agent operates based on fixed rules and deterministic actions, the addition of randomness introduces an element of exploration. This randomized agent has the ability to select actions randomly from its available options. As a result, it is more likely to explore and choose locations or actions that the simple reflex agent might overlook due to its rigid rule-based nature.\n",
    "\n",
    "By incorporating randomness into decision-making, the randomized agent gains an advantage in dynamic and uncertain environments. It can discover alternative paths or solutions that the simple reflex agent may not consider, thus potentially leading to better overall performance.\n",
    "\n",
    "\n",
    "Following is a graph with the results obtained: \n",
    "\n",
    "<div>\n",
    "<img src=\"Picture1.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "3. Can you design an environment in which your randomized agent will perform poorly? Show your results.\n",
    "\n",
    "Yes an environment with a given path will greatly reduce the performance. Following is a gif that portrais an example using the GUI provided.\n",
    "\n",
    "<div>\n",
    "<img src=\"Untitled video.gif\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "4. Can a reflex agent with state outperform a simple reflex agent? Design such an agent and measure its performance on several environments. Can you design a rational agent of this type?\n",
    "\n",
    "Yes, a reflex agent with state can outperform a simple reflex agent in some cases. For example, our design includes a `'wall hugger'` agent that stores previous locations and avoids them by marking them as `'Bump'`, leading to more efficient navigation. However, in some scenarios, the `'wall hugger'` agent can encounter its own tail, and if there is a wall, the agent may get stuck since it does not have the ability to retrace its steps to explore new locations. Despite this problem, evidence gathered suggests that our design for this agent is more than capable of surpassing a simple reflex agent in most scenarios.\n",
    "\n",
    "<div>\n",
    "<img src=\"Picture2.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "343fac69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T18:54:18.675422Z",
     "start_time": "2024-03-07T18:54:18.667326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "280\n"
     ]
    }
   ],
   "source": [
    "x = 6\n",
    "y = 6\n",
    "env0 = VacuumEnvironment(x, y)\n",
    "env1 = VacuumEnvironment(x, y)\n",
    "\n",
    "for i in range(10):\n",
    "    env0.add_thing(Dirt(), location=(random.randint(2,(x-2)), random.randint(2,(y-2))))\n",
    "    env1.add_thing(Dirt(), location=(random.randint(2,(x-2)), random.randint(2,(y-2))))\n",
    "\n",
    "for i in range(random.randint(1, 3)):\n",
    "    env0.add_thing(Wall(), location=(random.randint(2,(x-2)), random.randint(2,(y-2))))\n",
    "    env1.add_thing(Wall(), location=(random.randint(2,(x-2)), random.randint(2,(y-2))))\n",
    "\n",
    "def SimpleReflexAgent(xyEnv):\n",
    "    agt = XYReflexAgent(program=XYReflexAgentProgram)\n",
    "    env1.add_thing(agt, location=(3,3))\n",
    "    start_dirt = {len([x for x in env1.things if isinstance(x, Dirt)])}\n",
    "    #TraceAgent(agt)\n",
    "    #print('\\n''\\n')\n",
    "    env1.run(20)\n",
    "    end_dirt = {len([x for x in env1.things if isinstance(x, Dirt)])}\n",
    "    #print(f'Original amount of dirt in room: {start_dirt}''\\n''\\n')\n",
    "    #print(f'Dirt remaining in room: {end_dirt}''\\n''\\n')\n",
    "    print(agt.performance)\n",
    "\n",
    "def StatusReflexAgent(xyEnv):\n",
    "    agt = XYReflexAgent(program=XYStatusReflexAgentProgram)\n",
    "    env0.add_thing(agt, location=(3,3))\n",
    "    start_dirt = {len([x for x in env0.things if isinstance(x, Dirt)])}\n",
    "    #TraceAgent(agt)\n",
    "    #print('\\n''\\n')\n",
    "    env0.run(20)\n",
    "    end_dirt = {len([x for x in env0.things if isinstance(x, Dirt)])}\n",
    "    #print(f'Original amount of dirt in room: {start_dirt}''\\n''\\n')\n",
    "    #print(f'Dirt remaining in room: {end_dirt}''\\n''\\n')\n",
    "    print(agt.performance)\n",
    "\n",
    "StatusReflexAgent(env0)\n",
    "\n",
    "SimpleReflexAgent(env1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
